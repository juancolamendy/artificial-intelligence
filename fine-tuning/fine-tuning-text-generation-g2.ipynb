{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fine-tuning-text-generation-g2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOEaRgQpetziSDoMYrSGdrx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZTMB1gvNs3me"},"source":["#Source\n","https://github.com/falloutdurham/beginners-pytorch-deep-learning/blob/master/chapter9/Chapter9.5.ipynb\n","https://medium.com/swlh/fine-tuning-gpt-2-for-magic-the-gathering-flavour-text-generation-3bafd0f9bb93"]},{"cell_type":"markdown","metadata":{"id":"wkQnh6HBBiCr"},"source":["## Install dependencies"]},{"cell_type":"code","metadata":{"id":"ih-e0SkFBTT0"},"source":["!pip install --quiet transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yX4gc48TaGjz"},"source":["import os\n","\n","from tqdm import tqdm, trange\n","import numpy as np\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WKZK-XrCbAFN"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"R3liYy-VaqPh"},"source":["data = [\n","        \"Do one scary thing every day.Today, I reached out to 40+ PermanentLink trial users who didn't convert to paying customers.Writing those emails was harder than I thought, but the responses I already received show that it's worth it.\",\n","        \"In all my years of sales, nothing can stand up to the breadth of enrichment data Clearbit can provide. Still, there are gaps, especially outside the US. Are there any similar enrichment tools you trust?\"\n","        \"How to design almost any UI element.\",\n","        \"Our cold email warm-up system is rockin. 750 accounts warming up right now and 71,800 warm-up emails exchanged between these accounts in the last 10 days. You can now manage your settings and see stats from the dashboard.\",\n","        \"You can’t hack building relationships. It’s not like a podcast that you can run at 2x speed. The hack is consistency. Show up every day, contribute every day, and give people time to fall in love with your generosity.\",\n","        \"I could spend hours playing with dark mode UIs. There's just something awesome about dark mode.\"\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YGcVnerfD5i"},"source":["## Loader\n","We need to create a structured dataset and dataloader to appropriately feed into the model.\n","We will use in-built PyTorch classes to define the dataset and dataloader, which will feed the neural network.\n","* The dataset object will create a new list, which is a tuple of tensors.\n","* The first tensor is the encoded flavour text, wrapped in a start of text token, an end of text token and padded up to a maximum embedding length\n","* The second tensor is an attention mask, which is a list of 1's and 0's that tells the model which tokens are important\n","\n","This CustomDataSet can be generalized to fit any tokenizer and datalist.\n","\n","We're going to be using gpt2-small in this chapter, which has that limitation due to its hidden dimensionality of 768 (if you want to use larger pre-trained models, then you can increase this: gpt2-medium/1024, gpt2-large/1280, gpt2-xl/1600). Of course, because this dataset is only tweets, we're never going to bump up against the limit, but I thought I would I'd include it so you know to be aware of the limitation."]},{"cell_type":"code","metadata":{"id":"AQxzUjMZeume"},"source":["class CustomDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=768):\n","        self.tokenizer = tokenizer\n","        self.input_ids = []\n","        self.attn_masks = []\n","        \n","        for i in data:\n","          encs = tokenizer.encode_plus('<|startoftext|>'+i+'<|endoftext|>')\n","          self.input_ids.append( torch.tensor(encs['input_ids']) )\n","          self.attn_masks.append( torch.tensor(encs['attention_mask']) )        \n","\n","        print('--- input_ids')\n","        print(self.input_ids)\n","        print('--- attn_masks')\n","        print(self.attn_masks)\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.attn_masks[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVYX522DodQo"},"source":["## Global objects"]},{"cell_type":"code","metadata":{"id":"yryiDo8lyqQH"},"source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","dataset = CustomDataset(data, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vRoz9BZnop9R"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"5jhtAJSrpqAY"},"source":["Training GPT-2's involves passing our input text into the transformer model…and training the model to get the text back as output.\n","\n","As for our training loop, given that our labels are our input, all we're really doing is:\n","\n","```\n","outputs = model(input)\n","loss = loss_function(output, input)\n","loss.backward()\n","optimizer.step()\n","```\n"]},{"cell_type":"code","metadata":{"id":"I-xjGbvkp94m"},"source":["def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n","    if packed_tensor is None:\n","        return new_tensor, True, None\n","    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n","        return packed_tensor, False, new_tensor\n","    else:\n","        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n","        return packed_tensor, True, None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_D9923Ydnqq8"},"source":["def train(\n","    dataset,\n","    model,\n","    tokenizer,\n","    batch_size=16,\n","    epochs=4,\n","    lr=2e-5,\n","    max_seq_len=400,\n","    warmup_steps=5000,\n","    gpt2_type=\"gpt2\",\n","    device=\"cuda\",\n","    output_dir=\".\",\n","    output_prefix=\"wreckgar\",\n","    test_mode=False,\n","    save_model_on_epoch=False,\n","):\n","\n","    acc_steps = 100\n","\n","    model = model.to(device)\n","    model.train()\n","\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n","    )\n","\n","    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","    accumulating_batch_count = 0\n","    input_tensor = None\n","\n","    for epoch in range(epochs):\n","        print(f\"Training epoch {epoch}\")\n","        for idx, batch in enumerate(train_dataloader):\n","            binputs = batch[0].to(device)\n","            blabels = batch[0].to(device)\n","            bmasks = batch[1].to(device)\n","            outputs = model(binputs, labels=blabels, attention_mask=bmasks, token_type_ids=None)\n","            loss = outputs[0]\n","            loss.backward()\n","\n","            if (accumulating_batch_count % batch_size) == 0:\n","                optimizer.step()\n","                scheduler.step()\n","                optimizer.zero_grad()\n","                model.zero_grad()\n","\n","            accumulating_batch_count += 1\n","            input_tensor = None\n","        if save_model_on_epoch:\n","            torch.save(\n","                model.state_dict(),\n","                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n","            )\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzUgoozFr4vq"},"source":["!mkdir -p trained_models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExE6m8n5ntN3","executionInfo":{"status":"ok","timestamp":1618593435883,"user_tz":240,"elapsed":2240,"user":{"displayName":"Juan Carlos Olamendy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ5abnImE35Fu-BNSrmYlpO4C_dqQpuG2gjJmy5Q=s64","userId":"11004855458032977781"}},"outputId":"591bfb99-b90a-48f5-ecb6-849d29c75da9"},"source":["model = train(\n","    dataset,\n","    model,\n","    tokenizer,\n","    batch_size=16,\n","    epochs=1,\n","    lr=3e-5,\n","    max_seq_len=140,\n","    warmup_steps=5000,\n","    gpt2_type='gpt2',\n","    device=\"cuda\",\n","    output_dir=\"./trained_models\",\n","    output_prefix=\"twitter\",\n","    save_model_on_epoch=True\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training epoch 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-Tp8M6-csF2K"},"source":["def generate(\n","    model,\n","    tokenizer,\n","    prompt,\n","    entry_count=10,\n","    entry_length=100,\n","    top_p=0.8,\n","    temperature=1.,\n","):\n","\n","    model.eval()\n","\n","    generated_num = 0\n","    generated_list = []\n","\n","    filter_value = -float(\"Inf\")\n","\n","    with torch.no_grad():\n","\n","        for entry_idx in trange(entry_count):\n","\n","            entry_finished = False\n","\n","            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n","\n","            # Using top-p (nucleus sampling): https://github.com/huggingface/transformers/blob/master/examples/run_generation.py\n","\n","            for i in range(entry_length):\n","                outputs = model(generated, labels=generated)\n","                loss, logits = outputs[:2]\n","                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n","\n","                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","                cumulative_probs = torch.cumsum(\n","                    F.softmax(sorted_logits, dim=-1), dim=-1\n","                )\n","\n","                sorted_indices_to_remove = cumulative_probs > top_p\n","                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n","                    ..., :-1\n","                ].clone()\n","                sorted_indices_to_remove[..., 0] = 0\n","\n","                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","                logits[:, indices_to_remove] = filter_value\n","\n","                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n","                generated = torch.cat((generated, next_token), dim=1)\n","\n","                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n","                    entry_finished = True\n","\n","                if entry_finished:\n","\n","                    generated_num = generated_num + 1\n","\n","                    output_list = list(generated.squeeze().numpy())\n","                    output_text = tokenizer.decode(output_list)\n","\n","                    generated_list.append(output_text)\n","                    break\n","            \n","            if not entry_finished:\n","                output_list = list(generated.squeeze().numpy())\n","                output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n","                generated_list.append(output_text)\n","                \n","    return generated_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dzGmtmZHsQyp","executionInfo":{"status":"ok","timestamp":1618593968519,"user_tz":240,"elapsed":315085,"user":{"displayName":"Juan Carlos Olamendy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ5abnImE35Fu-BNSrmYlpO4C_dqQpuG2gjJmy5Q=s64","userId":"11004855458032977781"}},"outputId":"c046c1dd-3146-44cf-c9c0-1b0c28175477"},"source":["generated_tweets = generate(model, tokenizer, \"<|startoftext|>\",entry_count=10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [05:14<00:00, 31.47s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrpVOHnPuE3C","executionInfo":{"status":"ok","timestamp":1618594042517,"user_tz":240,"elapsed":281,"user":{"displayName":"Juan Carlos Olamendy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ5abnImE35Fu-BNSrmYlpO4C_dqQpuG2gjJmy5Q=s64","userId":"11004855458032977781"}},"outputId":"ddac1f4d-1d63-4b65-b084-74b4fdf06107"},"source":["print(generated_tweets)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['<|startoftext|>endofdocument</span> \".catch(title, endofdocument) <|replace \"<|replace \"#\", \" \"> \".format(endofdocument, \",replace))} \"<?xml version=\"1.0\" encoding=\"utf-8\"?> </body> </html> <?xml version=\"1.0\" encoding=\"utf-8\"?> </html>\\n\\nTutorial, here.<|endoftext|>', '<|startoftext|>=> []\\n\\nFor example, we could use Yarrow which exposes the QC_FAST__HS_FUNCTION_OPTS API.\\n\\nyarrow Yarrow :: CwdChar -> String\\n\\nReturn a vector which describes how to construct an edge edge. The position and outer values must be of type {W } with the Haskell type of Yarrow.\\n\\nclass Yarrow with M :: CwdChar -> S instance Yarrow a where X : [K ]<|endoftext|>', '<|startoftext|> <span>Hey Hey</span></body> </html>\\n\\n</html>\\n\\n<html>\\n\\n<head> <title>Enter the main html element</title>\\n\\n<link rel=\"stylesheet\" href=\"css/main.css\" />\\n\\n</head>\\n\\n<body>\\n\\n<div id=\"head\"></div>\\n\\n<script type=\"text/javascript\">\\n\\n(function() {\\n\\n(window) {\\n<|endoftext|>', '<|startoftext|>|<|printable|>|<|firstlastword|>|<|firstlastword|>|<|printable|>|<|firstlastword|>|<|firstlastword|>|<|printable|>|<|firstlastword|>|<|firstlastword|>|<|firstlastword|>|<|firstlastword|>|<|firstlastword|>|<|firstlastword|<|endoftext|>', '<|startoftext|>\\n\\n<|lastline|>\\n\\n<|firstline|>\\n\\n<|subsection|>\\n\\n<|lastbody|>\\n\\n<|last-child|>\\n\\n<|firstbody|>\\n\\n<|subsection|>\\n\\n<|last-child|>\\n\\n<|subsection|>\\n\\n<|last-child|>\\n\\n<|firstbody|>\\n\\n<|last-child|><|endoftext|>', '<|startoftext|>|rev|function|external|delete|>>|div|\\xadinform_escape_replace|newline|<--|footer|<<endofline|>|[</section>|]|<blockquote><\\\\/blockquote>|<quote><\\\\/div>|<<<<class=\"language\">[\\\\/color>]|<<END-MSG-XML-PREFIX|<\\\\/class><\\\\/body><\\\\/div>|<<endblockquote></<|endoftext|>', '<|startoftext|>|<l=^\\\\s*\\\\s*\\\\s*<|> <|}|<-~\\n\\nIf \"\\\\d+|\\\\d+\" were given in the empty string character, that would produce this value.\\n\\nFor words containing text, \\\\s + a or / (for instance, \\\\r, / for \\\\d, /^/ for \\\\s ) would produce a similar value.\\n\\n\\\\(<\\\\d+|>\\\\s<|endoftext|>', '<|startoftext|> I got ready to leave. (OTV, 8.2.13.23) #38\\n\\n\"first_line.mdt\" \"People (closest country to show up) ♥\"\\n\\n\"shopecore.mdt\" \"What kind of glass are you talking about? □\"\\n\\n\"fish.mdt\" \"One of the guys with big wings ♥\"\\n\\n\"orlando.mdt\" \"You\\'re living<|endoftext|>', '<|startoftext|><br /><br />This is a snippet of a TextEvent in the source file:</br /> <br />This text will trigger a new Event in the source. It can be any other type. In this example, TextEvent.source is: <br />{{ text-type:text-type | text-value:<input type=\"text\" data-type=\"x\"> <input type=\"text\" data-type=\"y\"> <input type=\"text\" data<|endoftext|>', '<|startoftext|>\\n\\nThis is the algorithm to find the poem. You have to first find the individual line on the page.\\n\\nIt is important to look at the number of lines before you figure out the placement of the preceding words in the sentence.\\n\\nThis is the number of dots. Note that only when the same line occurs twice in a sentence does the algorithm correct, because each line contains the same number of dots.\\n\\nSaying that is the location of the first line does not make<|endoftext|>']\n"],"name":"stdout"}]}]}