{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"translation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNfqjK8DoNPXEOMRkGEZuh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZinXOKW5cupe"},"source":["# Install deps"]},{"cell_type":"code","metadata":{"id":"GvNvuNrgcdQS"},"source":["!pip install transformers\n","!pip install torch\n","!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_x4jo0xcxPu"},"source":["# Import deps"]},{"cell_type":"code","metadata":{"id":"t7PJ5AI_cz9D","executionInfo":{"status":"ok","timestamp":1635174550067,"user_tz":240,"elapsed":28851,"user":{"displayName":"Juan Carlos Olamendy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ5abnImE35Fu-BNSrmYlpO4C_dqQpuG2gjJmy5Q=s64","userId":"11004855458032977781"}}},"source":["from transformers import MarianMTModel, MarianTokenizer\n","from typing import List"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXyIe3lNdDOP"},"source":["src = 'es'  # source language\n","trg = 'en'  # target language\n","mname = f'Helsinki-NLP/opus-mt-{src}-{trg}'\n","\n","# pre-trained model\n","print(mname)\n","model = MarianMTModel.from_pretrained(mname)\n","tokenizer = MarianTokenizer.from_pretrained(mname)\n","\n","# tokenizer and model\n","# print(tokenizer)\n","# print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_XsohcUdMEb","executionInfo":{"status":"ok","timestamp":1635174933840,"user_tz":240,"elapsed":662,"user":{"displayName":"Juan Carlos Olamendy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ5abnImE35Fu-BNSrmYlpO4C_dqQpuG2gjJmy5Q=s64","userId":"11004855458032977781"}},"outputId":"f787c343-a36f-43d9-e7eb-af5930a7a91e"},"source":["text = \"Â¿Donde esta la muchacha?\"\n","input_ids = tokenizer.encode(text, return_tensors=\"pt\", padding=True)\n","outputs = model.generate(input_ids)\n","decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(decoded)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Where's the girl?\n"]}]}]}