{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPdPKBmdVDXsI78bpUoeYbt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"h-3PZzkRYc5C"},"source":["# Bert Classification\n","[Documentation](https://medium.com/analytics-vidhya/text-classification-with-bert-using-transformers-for-long-text-inputs-f54833994dfd)"]},{"cell_type":"code","metadata":{"id":"2G9jkCvuYb5V"},"source":["import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","import torch\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","from textwrap import wrap\n","\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device\n","\n","def prepare_reduced_data(data):\n","    df=data[10000:14000]\n","    df=df.set_index(pd.Series(list(range(4000))))\n","    df.loc[(df.doc_class=='Unacceptable'), 'doc_class']=0\n","    df.loc[(df.doc_class=='Acceptable'), 'doc_class']=1\n","    return df\n","\n","def defining_bert_tokenizer(PRE_TRAINED_MODEL_NAME):\n","    tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","    return tokenizer\n","\n","class GPReviewDataset(Dataset):\n","    \n","    def __init__(self, doc, targets, tokenizer, max_len):\n","        self.doc = doc\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","    \n","    def __len__(self):\n","        return len(self.doc)\n","  \n","    def __getitem__(self, item):\n","        doc = str(self.doc[item])\n","        target = self.targets[item]\n","\n","        encoding = self.tokenizer.encode_plus(\n","                    doc,\n","                    add_special_tokens=True,\n","                    max_length=self.max_len,\n","                    return_token_type_ids=False,\n","                    pad_to_max_length=True,\n","                    return_attention_mask=True,\n","                    return_tensors='pt',\n","                    )\n","\n","        return {\n","            'doc_text': doc,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'targets': torch.tensor(target, dtype=torch.long)\n","            }\n","\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size):\n","    ds = GPReviewDataset(\n","        doc=df.encounter_text.to_numpy(),\n","        targets=df.doc_class.to_numpy(),\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","      )\n","    \n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        num_workers=4\n","        )\n","\n","class SentimentClassifier(nn.Module):\n","    def __init__(self, n_classes):\n","        super(SentimentClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","        self.drop = nn.Dropout(p=0.3)\n","        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n","    \n","    def forward(self, input_ids, attention_mask):\n","        _, pooled_output = self.bert(\n","          input_ids=input_ids,\n","          attention_mask=attention_mask\n","        )\n","        output = self.drop(pooled_output)\n","        return self.out(output)\n","\n","def train_epoch(\n","  model, \n","  data_loader, \n","  loss_fn, \n","  optimizer, \n","  device, \n","  scheduler, \n","  n_examples\n","    ):\n","    model = model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","\n","    for d in data_loader:\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        targets = d[\"targets\"].to(device)\n","\n","        outputs = model(\n","          input_ids=input_ids,\n","          attention_mask=attention_mask\n","        )\n","\n","        _, preds = torch.max(outputs, dim=1)\n","        loss = loss_fn(outputs, targets)\n","\n","        correct_predictions += torch.sum(preds == targets)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model = model.eval()\n","\n","    losses = []\n","    correct_predictions = 0\n","\n","    with torch.no_grad():\n","        for d in data_loader:\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            targets = d[\"targets\"].to(device)\n","\n","            outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","            )\n","            _, preds = torch.max(outputs, dim=1)\n","\n","            loss = loss_fn(outputs, targets)\n","\n","            correct_predictions += torch.sum(preds == targets)\n","            losses.append(loss.item())\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def get_predictions(model, data_loader):\n","    model = model.eval()\n","\n","    review_texts = []\n","    predictions = []\n","    prediction_probs = []\n","    real_values = []\n","\n","    with torch.no_grad():\n","        for d in data_loader:\n","\n","            texts = d[\"doc_text\"]\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            targets = d[\"targets\"].to(device)\n","\n","            outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","            )\n","            _, preds = torch.max(outputs, dim=1)\n","\n","            probs = F.softmax(outputs, dim=1)\n","\n","            review_texts.extend(texts)\n","            predictions.extend(preds)\n","            prediction_probs.extend(probs)\n","            real_values.extend(targets)\n","\n","    predictions = torch.stack(predictions).cpu()\n","    prediction_probs = torch.stack(prediction_probs).cpu()\n","    real_values = torch.stack(real_values).cpu()\n","    return review_texts, predictions, prediction_probs, real_values\n","\n","\n","if __name__==\"__main__\":\n","    \n","    full_data=pd.read_csv('./data/training_data_cleaned_wo_oua_10_05_2020.csv')\n","    df=prepare_reduced_data(full_data)\n","\n","    class_names=['Unacceptable', 'Acceptable']\n","    PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n","    tokenizer=defining_bert_tokenizer(PRE_TRAINED_MODEL_NAME)\n","\n","    sample_txt='Personal Health Record (Extract)\\nCreated on October 24, 2019\\nPatient\\nSteven Fuerst\\nBirthdate\\nDecember 10, 1979\\nRace\\nInformation not\\navailable'\n","\n","    tokens = tokenizer.tokenize(sample_txt)\n","    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","    encoding = tokenizer.encode_plus(\n","                      sample_txt,\n","                      max_length=64,\n","                      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n","                      return_token_type_ids=False,\n","                      pad_to_max_length=True,\n","                      return_attention_mask=True,\n","                      return_tensors='pt',  # Return PyTorch tensors\n","                    )\n","\n","    \n","    \n","    MAX_LEN = 128\n","    BATCH_SIZE = 16\n","\n","    df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n","    df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n","\n","    train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","    val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n","    test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n","\n","    data = next(iter(train_data_loader))\n","\n","    bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","\n","    last_hidden_state, pooled_output = bert_model(\n","                    input_ids=encoding['input_ids'], \n","                    attention_mask=encoding['attention_mask'])\n","\n","    model = SentimentClassifier(len(class_names))\n","    model = model.to(device)\n","\n","    input_ids = data['input_ids'].to(device)\n","    attention_mask = data['attention_mask'].to(device)\n","\n","\n","    EPOCHS = 10\n","\n","    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","    total_steps = len(train_data_loader) * EPOCHS\n","\n","    scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n","    )\n","\n","    loss_fn = nn.CrossEntropyLoss().to(device)\n","\n","\n","    history = defaultdict(list)\n","    best_accuracy = 0\n","\n","    for epoch in range(EPOCHS):\n","\n","        print(f'Epoch {epoch + 1}/{EPOCHS}')\n","        print('-' * 10)\n","\n","        train_acc, train_loss = train_epoch(\n","        model,\n","        train_data_loader,    \n","        loss_fn, \n","        optimizer, \n","        device, \n","        scheduler, \n","        len(df_train)\n","        )\n","\n","        print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","        val_acc, val_loss = eval_model(\n","        model,\n","        val_data_loader,\n","        loss_fn, \n","        device, \n","        len(df_val)\n","        )\n","\n","        print(f'Val   loss {val_loss} accuracy {val_acc}')\n","        print()\n","\n","        history['train_acc'].append(train_acc)\n","        history['train_loss'].append(train_loss)\n","        history['val_acc'].append(val_acc)\n","        history['val_loss'].append(val_loss)\n","\n","        if val_acc > best_accuracy:\n","            torch.save(model.state_dict(), 'best_model_state.bin')\n","            best_accuracy = val_acc\n","        \n","    test_acc, _ = eval_model(\n","          model,\n","          test_data_loader,\n","          loss_fn,\n","          device,\n","          len(df_test)\n","        )\n","    print('\\nTest Accuracy:\\n')\n","    print(test_acc.item())\n","\n","    y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n","    model,\n","    test_data_loader\n","    )\n","\n","    print(classification_report(y_test, y_pred, target_names=class_names))\n"],"execution_count":null,"outputs":[]}]}