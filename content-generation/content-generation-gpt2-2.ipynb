{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"content-generation-gpt2-2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNAgnvKC6ed+2nf2MJehWk9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BCnmjiq8AmY3"},"source":["# Install dep"]},{"cell_type":"code","metadata":{"id":"f7Cu9Bm9Ac5P"},"source":["!pip install --quiet transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i66wcZDmAzQG"},"source":["# Import dep"]},{"cell_type":"code","metadata":{"id":"5PN-QtRRA9ZB","executionInfo":{"status":"ok","timestamp":1622911165476,"user_tz":240,"elapsed":89,"user":{"displayName":"Juan Carlos Olamendy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ5abnImE35Fu-BNSrmYlpO4C_dqQpuG2gjJmy5Q=s64","userId":"11004855458032977781"}}},"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering\n","import torch\n","from torch.nn import functional as F"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zBJx-rxgBFlP"},"source":["# Init and load models/tokenizers"]},{"cell_type":"code","metadata":{"id":"BK8wgAO1BHcx"},"source":["# Load GPT-2 tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","model = AutoModelForCausalLM.from_pretrained('gpt2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FpCjnavNBY0H"},"source":["# Logic"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMYfGzgUBbap","executionInfo":{"status":"ok","timestamp":1622910947185,"user_tz":240,"elapsed":459,"user":{"displayName":"Juan Carlos Olamendy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ5abnImE35Fu-BNSrmYlpO4C_dqQpuG2gjJmy5Q=s64","userId":"11004855458032977781"}},"outputId":"78f59d46-8570-4afa-bf54-d7b108bf599b"},"source":["# Tokenize input phrase\n","phrase = f'I sleep in a bed that is poorly iced, it\\'s okay, it goes away. I\\'ll make tea for you'\n","inputs = tokenizer.encode(phrase, return_tensors='pt')\n","print(inputs)\n","\n","# Get logits from last layer\n","last_layer_logits = model(inputs).logits[:, -1, :]\n","\n","# Keep top 30 logits at max; stop if cumulative probability >= 1.0.\n","top_logits = top_k_top_p_filtering(last_layer_logits, top_k=30, top_p=1.0)\n","\n","# Softmax the logits into probabilities\n","probabilities = F.softmax(top_logits, dim=-1)\n","\n","# Generate next token\n","generated_next_token = torch.multinomial(probabilities, num_samples=1)\n","generated = torch.cat([inputs, generated_next_token], dim=-1)\n","\n","# Get result\n","result_string = tokenizer.decode(generated.tolist()[0])\n","\n","# Print string\n","print(result_string)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["tensor([[   40,  3993,   287,   257,  3996,   326,   318, 13455,   220,  3711,\n","            11,   340,   338,  8788,    11,   340,  2925,  1497,    13,   314,\n","          1183,   787,  8887,   329,   345]])\n","I sleep in a bed that is poorly iced, it's okay, it goes away. I'll make tea for you,\n"],"name":"stdout"}]}]}